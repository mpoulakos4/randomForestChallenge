---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

## Prerequisite: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance for this dataset.


```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```

## Results: The Power of Ensemble Learning
### Performance Trends

```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

## Question 1: The Power of More Trees 

**Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

```{r}
#| label: power-of-trees-viz
#| echo: false
#| fig-width: 12
#| fig-height: 8
#| message: false
#| warning: false

# Create visualization showing the power of more trees
library(ggplot2)
library(gridExtra)

# Prepare data for plotting
plot_data <- performance_df %>%
  pivot_longer(cols = c(RMSE_Test, RMSE_Train), 
               names_to = "Dataset", 
               values_to = "RMSE") %>%
  mutate(Dataset = ifelse(Dataset == "RMSE_Test", "Test Data", "Training Data"))

# RMSE plot
p1 <- ggplot(plot_data, aes(x = Trees, y = RMSE, color = Dataset)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  scale_x_log10() +
  labs(title = "RMSE vs Number of Trees",
       subtitle = "Lower RMSE indicates better performance",
       x = "Number of Trees (log scale)",
       y = "Root Mean Square Error",
       color = "Dataset") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12))

# R-squared plot
p2 <- ggplot(performance_df, aes(x = Trees, y = R_squared)) +
  geom_line(size = 1.2, color = "darkblue") +
  geom_point(size = 2, color = "darkblue") +
  scale_x_log10() +
  labs(title = "R-squared vs Number of Trees",
       subtitle = "Higher R-squared indicates better model fit",
       x = "Number of Trees (log scale)",
       y = "R-squared") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12))

# Combine plots
grid.arrange(p1, p2, ncol = 2)
```

**Analysis of the Power of More Trees:**

The visualization reveals several key insights about ensemble learning. This first being that there is dramitic early improvements. This can be seen through the most significant performance gains occuring in the first 25 trees. There is a 32% RMSE improvement from 1 tree to 25 trees. This means that the model is able to explain 32% more of the variance in the data with 25 trees than with 1 tree (predictions are closer to the actual values). This can also be seen in the R-squared, which jumps from 0.65 to 0.85, showing an incraese in explanitory power as the number of trees increases. After 25 trees, the improvements become much more modest. Moving from 25 to 100 trees only reduces RMSE by about $1,000 (3% improvement), and from 100 to 1000 trees by less than $500 (1.5% improvement). This demonstrates the classic pattern of diminishing returns in ensemble methods. This is a result of the bootstrap sampling and random feature selection that are used to build the trees. As the number of trees increases, the trees become more and more similar, and the performance gains decrease (less new information as the forest grows).

## Question 2: Overfitting Visualization and Analysis

**Task:** Compare decision trees vs random forests in terms of overfitting.

```{r}
#| label: overfitting-analysis
#| echo: false
#| fig-width: 14
#| fig-height: 8
#| message: false
#| warning: false

# Create decision trees with different max depths to show overfitting
library(rpart)

# Function to calculate RMSE
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Test different max depths for decision trees
max_depths <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
tree_rmse_train <- numeric(length(max_depths))
tree_rmse_test <- numeric(length(max_depths))

for (i in 1:length(max_depths)) {
  # Build decision tree with specific max depth
  tree_model <- rpart(SalePrice ~ ., data = train_data, 
                      control = rpart.control(maxdepth = max_depths[i]))
  
  # Calculate predictions
  train_pred <- predict(tree_model, train_data)
  test_pred <- predict(tree_model, test_data)
  
  # Calculate RMSE
  tree_rmse_train[i] <- calculate_rmse(train_data$SalePrice, train_pred)
  tree_rmse_test[i] <- calculate_rmse(test_data$SalePrice, test_pred)
}

# Create data frames for plotting
tree_data <- data.frame(
  Complexity = max_depths,
  RMSE_Train = tree_rmse_train,
  RMSE_Test = tree_rmse_test
)

rf_data <- performance_df %>%
  select(Trees, RMSE_Train, RMSE_Test) %>%
  rename(Complexity = Trees)

# Create side-by-side plots
library(gridExtra)

# Decision tree plot
p_tree <- ggplot(tree_data, aes(x = Complexity)) +
  geom_line(aes(y = RMSE_Train, color = "Training"), size = 1.2) +
  geom_line(aes(y = RMSE_Test, color = "Test"), size = 1.2) +
  geom_point(aes(y = RMSE_Train, color = "Training"), size = 2) +
  geom_point(aes(y = RMSE_Test, color = "Test"), size = 2) +
  labs(title = "Decision Trees: Overfitting with Complexity",
       subtitle = "Training performance improves while test performance degrades",
       x = "Max Depth",
       y = "RMSE",
       color = "Dataset") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12)) +
  scale_color_manual(values = c("Training" = "blue", "Test" = "red"))

# Random forest plot
p_rf <- ggplot(rf_data, aes(x = Complexity)) +
  geom_line(aes(y = RMSE_Train, color = "Training"), size = 1.2) +
  geom_line(aes(y = RMSE_Test, color = "Test"), size = 1.2) +
  geom_point(aes(y = RMSE_Train, color = "Training"), size = 2) +
  geom_point(aes(y = RMSE_Test, color = "Test"), size = 2) +
  scale_x_log10() +
  labs(title = "Random Forests: No Overfitting with More Trees",
       subtitle = "Both training and test performance improve together",
       x = "Number of Trees (log scale)",
       y = "RMSE",
       color = "Dataset") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12)) +
  scale_color_manual(values = c("Training" = "blue", "Test" = "red"))

# Combine plots
grid.arrange(p_tree, p_rf, ncol = 2)
```

**Analysis of Overfitting in Decision Trees vs Random Forests:**

Decision Trees Suffer from overfitting, espeically as they become more complex. As individual decision trees become more complex (higher max depth), they show a classic overfitting pattern. Training RMSE continues to improve, reaching very low values, while test RMSE actually gets worse after depth 4-5. This happens because individual trees memorize the training data rather than learning generalizable patterns. When these trees memorize the training data, they are no longer able to generalize to new data, and the performance on the test data suffers which causes the overfitting.

Random forests on the other hand, do not suffer from overfitting. They do this by aggregating diverse trees and introducing "controlled randomness" that prevents any single tree from memorizing the training data. This is why random forests are able to generalize better to new data, and why they are able to improve performance as the number of trees increases. Specifically, bootstrap sampling trains each tree on a different random sample, preventing any single tree from memorizing the entire dataset. Also, the random feature selection only considers a random subset of features at each split, forcing trees to learn different patterns and reducing correlation between trees. On top of this, by averaging predictions from many diverse trees, random forests smooth out individual tree biases and reduce variance. The randomness acts as a natural form of regularization, preventing the model from fitting noise in the training data.

## Question 3: Linear Regression vs Random Forest Comparison

**Your Task:** Compare random forests to linear regression baseline.

```{r}
#| label: linear-regression-comparison
#| echo: false
#| message: false
#| warning: false

# Build linear regression model
lm_model <- lm(SalePrice ~ ., data = train_data)
lm_predictions <- predict(lm_model, test_data)
lm_rmse <- sqrt(mean((test_data$SalePrice - lm_predictions)^2))

# Get RMSE values for specific random forest models
rf_1_rmse <- performance_df$RMSE_Test[performance_df$Trees == 1]
rf_100_rmse <- performance_df$RMSE_Test[performance_df$Trees == 100]
rf_1000_rmse <- performance_df$RMSE_Test[performance_df$Trees == 1000]

# Calculate percentage improvements over linear regression
rf_1_improvement <- ((lm_rmse - rf_1_rmse) / lm_rmse) * 100
rf_100_improvement <- ((lm_rmse - rf_100_rmse) / lm_rmse) * 100
rf_1000_improvement <- ((lm_rmse - rf_1000_rmse) / lm_rmse) * 100

# Create comparison table
comparison_table <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  RMSE = c(lm_rmse, rf_1_rmse, rf_100_rmse, rf_1000_rmse),
  Improvement_over_LR = c("Baseline", paste0(round(rf_1_improvement, 1), "%"), 
                          paste0(round(rf_100_improvement, 1), "%"), 
                          paste0(round(rf_1000_improvement, 1), "%")),
  Complexity = c("Low", "Low", "Medium", "High"),
  Interpretability = c("High", "High", "Low", "Low")
)

# Display the table
knitr::kable(comparison_table, 
              caption = "Model Performance Comparison: Linear Regression vs Random Forests",
              col.names = c("Model", "RMSE", "Improvement over Linear Regression", "Complexity", "Interpretability"),
              digits = 0)
```

**Analysis of Linear Regression vs Random Forest Performance:**

When we go from 1 tree to 100 trees, we see a significant improvement in the RMSE. The RMSE drops from $47,234 to $33,300, a 29.5% improvement. This means that the model is able to explain 29.5% more of the variance in the data with 100 trees than with 1 tree (predictions are closer to the actual values). Switching from linear regression to 100-tree random forest shows similar improvements, with a 27.4% improvement in RMSE from $45,891 to $33,300. This suggests that the ensemble effect is the primary driver of performance gains. Random forests are worth the complexity when non-linear relationships are suspected, feature interactions are important, performance is critical, and black box predictions are acceptable. You should stick with linear regression when interpretability is crucial (business decisions), computational resources are limited, and quick prototyping is needed. There are also some trade-offs to consider. Random forests are a black box model, meaning that it is difficult to understand how the model is making predictions. There are also more hyperparameters to tune and a more complex deployment. However, they do does have stronger accuracy. Linear regression is a more interpretable model, meaning that it is easier to understand how the model is making predictions since it is a fairly simple deployment and minimal tuning is needed. However it does have weaker accuracy. Overall, the decision should be based on the specific needs of the problem and the trade-offs between interpretability and accuracy.

